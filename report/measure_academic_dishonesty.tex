\documentclass{article}

\usepackage[margin=1in]{geometry}

\usepackage{graphicx}

\begin{document}
    \author{Kavi Gupta}
    \title{61A Exam Data Analysis: Measure of Academic Dishonesty}
    \maketitle

\section{General Strategy}
    The general strategy is to look at similarities between scores of students sitting next to each
        other against the similarities between scores of students sitting not next to each other.

    The result can be then used to estimate the number of students committing academic dishonesty
        using a model.

\section{Confounding Factors}
    There are many potential confounding factors, which we control for in different ways.
    \subsection{Grader Differences}
        Some graders grade differently from each other. Taking Midterm 1 problem 1.3, we can see that
            graders 5 and 8 had fairly typical behavior in terms of average scores given. However, we can
            also see that they have a very different profile of rubric items. In this case, what happened
            was that these two graders were giving rubric items 2, 3, and 4, which corresponded to each of
            the three parts of the problem being correct, rather than giving rubric item 1, which
            corresponded to the entire problem being correct. While this is irrelevant to the student, it
            does confound analyses.

        \begin{figure}[h!]
            \centering
            \includegraphics[width=\textwidth]{img/grader-comparison.png}
        \end{figure}

        Since grading ranges tend to be at least somewhat associated with location, this might lead to
            students near each other having more similar profiles, artificially. We control for graders
            by subtracting out from each student's individual problem scores and rubric items the mean
            given by a grader.

        Any student who had at least one problem that was either graded by a grader who had graded fewer
            than ten problems (not enough of a track record) or who had an abnormal pattern of rubric item
            grades\footnote{Unusualness of a grader is defined as
                $$u = \sum_k \left|\frac{x_k - \mu_k}{\sigma_k}\right|$$ where $x_k$ is the mean for the
                given grader of the $k$th rubric item, and $\mu_k, \sigma_k$ are the mean and standard
                deviation for that rubric item in general}.
    \subsection{Sequencing}
        The Gambler's Fallacy could possibly lead to a negative correlation between consecutively graded
            exams. To control for this, we simply ignore pairs of exams that are graded near each other in
            time.
    \subsection{Room}
        Individual rooms might have different rubric profiles, due to, for example, different TAs in
            different rooms. It looks like no particular room has that much of a problem, so for now
            rooms are not controlled for except to not take pairs from different rooms (there is a
            small amount of variation).
        \begin{figure}[h!]
            \centering
            \includegraphics[width=\textwidth]{img/room-comparison.png}
        \end{figure}
    \subsection{Front of Room / Back of Room}
        Sections within rooms appear to have little effect on exam profiles (at least within the
            margin of error), and definitely nothing systematic.
        \begin{figure}[h!]
            \centering
            \includegraphics[width=\textwidth]{img/region-comparison.png}
        \end{figure}
    \subsection{Aisle / Middle}
        No analysis on this factor as of yet. Trying to find aisle locations is impossible from
            seating chart data; would require some data gathering.
    \subsection{Seating Chart Inaccuracies}
        Not yet fixed. Have the data, but would require a lot of data input because OCR is not particularly
            effective at recognizing handwriting.
\section{Measuring Similarity}
    \subsection{Correlation}
        Using correlation between normalized scores as a measure of similarity, we can prove that
            there is a difference between the similarities in the set of adjacent pairs of students
            and the set of non-adjacent pairs of students (see controls above for more details).

        Overall, a permutation test shows that the difference is in fact significant:
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.5\textwidth]{img/permutation-test-correlation.png}
        \end{figure}
    \subsection{Absolute Difference in Overall Exam Score}
        An alternate means of similarity is far simpler. We look at the average difference
            between the scores in the two exams. It turns out that despite its unsophistication,
            this test is also able to see past the noise in the data.
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.5\textwidth]{img/permutation-test-abs-difference.png}
        \end{figure}

        In this case, the difference is the opposite as above, because a higher correlation
            corresponds to a smaller absolute difference.
    \section{Model For Academic Dishonesty}
        \subsection{Score-Independent, Binary Cheater, Random Cheat-Selection, Model}
            Assume that some fixed fraction $c$ of the students cheat by copying exactly $k$
                randomly selected points worth of material from a person sitting next to them (also
                selected randomly). Assume that non-cheating students get each point independently
                with probability $p$. Further assume that cheating students are located randomly.

            Using known exam data, we can approximate $p$ simply by taking the average score divided
                by the total number of points.

            Unfortunately, the grades this model produces do not approximate the true distribution
                of grades, as the graph below demonstrates.

        \subsection{Question-Independent, Binary Cheater, Random Cheat-Selection Model}
            This model is equivalent to the one above, but uses point chunks representing each
                question, which are assumed to be independent and take the sizes of each of the
                questions. A certain number of questions rather than points are copied. Each question
                is assigned a score based on a normal distribution from the mean and variance of
                the real data for that question.

            Unfortunately, this strategy also produces a distribution of grades dissimilar to the
                original data.

            \begin{figure}[h!]
                \centering
                \includegraphics[width=\textwidth]{img/independents-not-working.png}
            \end{figure}

\end{document}
